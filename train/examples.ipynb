{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31396db",
   "metadata": {},
   "source": [
    "# Examples for Behavior Cloning Dataset Generation and Usage in Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9c5e8",
   "metadata": {},
   "source": [
    "This notebook contains convenient examples demonstrating use of dataset generation and training pipeline functions.\n",
    "\n",
    "1. Start off by running the first cell below.\n",
    "2. Then, proceed in order. Some cells may be able to be skipped depending on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e52206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.getcwd()).parent.absolute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84016c",
   "metadata": {},
   "source": [
    "### Generating a Bimanual Behavior Cloning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00caf7fc",
   "metadata": {},
   "source": [
    "Here, we generate datasets of (BimanualObs, BimanualAction) samples for the \"pickup block\" and \"pass block\" task using custom, privileged-knowledge policies.\n",
    "This takes a while, so I've implemented convenient mechanisms for resuming data generation between sessions.\n",
    "The resulting output files can be accessed again with the BimanualDataset class, demonstrated in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d79af6",
   "metadata": {},
   "source": [
    "##### *Pickup block* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy.privileged_pickup_block_policy import PrivilegedPickupBlockPolicy\n",
    "from robot.sim import BimanualSim\n",
    "from train.dataset import generate_bimanual_dataset\n",
    "from validate.evaluation import PickupBlockTaskEvaluator\n",
    "\n",
    "pickup_block_dataset_path = Path('D:/bimaminobolonana/pickup-block-dataset')\n",
    "generate_bimanual_dataset(\n",
    "  save_dir=pickup_block_dataset_path,\n",
    "  create_sim=lambda: BimanualSim(merge_xml_files=['block.xml'], camera_dims=(128, 128)),\n",
    "  create_privileged_policy=PrivilegedPickupBlockPolicy,\n",
    "  create_task_evaluator=PickupBlockTaskEvaluator,\n",
    "  total_sample_count=600,\n",
    "  max_steps_per_rollout=600,\n",
    "  skip_frames=0,\n",
    "  camera_dims=(128, 128),\n",
    "  resume=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6878f19",
   "metadata": {},
   "source": [
    "##### *Pass block* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ac692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy.privileged_pass_block_policy import PrivilegedPassBlockPolicy\n",
    "from robot.sim import BimanualSim, randomize_block_position\n",
    "from train.dataset import generate_bimanual_dataset\n",
    "from validate.evaluation import PassBlockTaskEvaluator\n",
    "\n",
    "pass_block_dataset_path = Path('D:/bimaminobolonana/pass-block-dataset')\n",
    "generate_bimanual_dataset(\n",
    "  save_dir=pass_block_dataset_path,\n",
    "  create_sim=lambda: BimanualSim(merge_xml_files=['block.xml'], camera_dims=(128, 128), on_mujoco_init=randomize_block_position),\n",
    "  create_privileged_policy=PrivilegedPassBlockPolicy,\n",
    "  create_task_evaluator=PassBlockTaskEvaluator,\n",
    "  total_sample_count=10000,\n",
    "  max_steps_per_rollout=600,\n",
    "  skip_frames=2,\n",
    "  camera_dims=(128, 128),\n",
    "  resume=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af73614",
   "metadata": {},
   "source": [
    "### Viewing a Rollout from the Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c45345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved left wrist video to out/left_wrist_rollout_0.mp4\n"
     ]
    }
   ],
   "source": [
    "from robot.visualize import save_frames_to_video\n",
    "from train.dataset import BimanualDataset\n",
    "pickup_block_dataset_path = Path('/mnt/data/simple-pickup-no-randomization')\n",
    "dataset = BimanualDataset(pickup_block_dataset_path)\n",
    "rollout_number = 0\n",
    "rollout_length = dataset.metadata.rollout_lengths[rollout_number]\n",
    "rollout_start = sum(dataset.metadata.rollout_lengths[:rollout_number])\n",
    "observations = [dataset[i][0] for i in range(rollout_start, rollout_start + rollout_length)]\n",
    "\n",
    "os.makedirs('out', exist_ok=True)\n",
    "left_wrist_video_path = f'out/left_wrist_rollout_{rollout_number}.mp4'\n",
    "right_wrist_video_path = f'out/right_wrist_rollout_{rollout_number}.mp4'\n",
    "save_frames_to_video([observation.visual[0, 0].detach().numpy() for observation in observations], left_wrist_video_path)\n",
    "save_frames_to_video([observation.visual[0, 1].detach().numpy() for observation in observations], right_wrist_video_path)\n",
    "print(f\"Saved left wrist video to {left_wrist_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ced06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"out/left_wrist_rollout_0.mp4\" controls  width=\"400\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(left_wrist_video_path, width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65d0e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"out/right_wrist_rollout_0.mp4\" controls  width=\"400\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(right_wrist_video_path, width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f50e5",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba432e9",
   "metadata": {},
   "source": [
    "Here we demonstrate the suite of utilities in this `train` subpackage.\n",
    "\n",
    "1. `train.train_utils.Logs`\n",
    "  - Utility class for organizing training output files. We should use this going forward to remain organized and expand its capabilities as needed.\n",
    "2. `train.dataset.BimanualDataset`\n",
    "  - Dataset implementation for reading behavior cloning data.\n",
    "3. `train.trainer.BCTrainer`\n",
    "  - A class for training an arbitrary model for behavior cloning. We should more-or-less keep this class's core logic as-is.\n",
    "\n",
    "Each of these utilities currently have some TODO comments in their code - take a look. In summary, (1. Logs) doesn't currently do much logging - the Jobs class in the same file needs to have log piping implemented. (2. Dataset) currently just uses robot.sim numpy-based dataclasses for observations and actions, it should start using the torch-based versions defined in dataset.py and moving the .npy files to GPU. (3. BCTrainer) isn't very configurable right now - the optimizer and other details should be constructor args that we can dynamically configure with hydra or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532cc965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n",
      "Training model for 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 40/40 [02:07<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 0 loss: 504.3760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 40/40 [00:01<00:00, 31.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 1 loss: 15.9850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 40/40 [00:01<00:00, 35.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 2 loss: 11.6654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 40/40 [00:01<00:00, 34.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 3 loss: 10.7553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 40/40 [00:01<00:00, 31.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 4 loss: 10.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 40/40 [00:01<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 5 loss: 9.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 40/40 [00:01<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 6 loss: 9.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 40/40 [00:01<00:00, 30.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 7 loss: 9.4972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 40/40 [00:01<00:00, 35.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 8 loss: 8.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 40/40 [00:01<00:00, 34.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 9 loss: 9.2741\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from robot.sim import JOINT_OBSERVATION_SIZE\n",
    "from train.dataset import BimanualDataset, TensorBimanualAction, TensorBimanualObs\n",
    "from train.train_utils import Logs\n",
    "from train.trainer import BCTrainer, BimanualActor\n",
    "\n",
    "# example bimanual actor class\n",
    "class ExampleModel(BimanualActor):\n",
    "  def __init__(self, observation_size: int, action_size: int, hidden_size: int = 256):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(observation_size, hidden_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_size, hidden_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_size, action_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, obs: TensorBimanualObs) -> TensorBimanualAction:\n",
    "    x = torch.cat((obs.visual.reshape(obs.visual.shape[0], -1), obs.qpos.array), dim=-1)\n",
    "    return TensorBimanualAction(self.layers(x))\n",
    "\n",
    "# load dataset and set up trainer\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "CHECKPOINT_FREQUENCY = 1\n",
    "dataset = BimanualDataset(pickup_block_dataset_path)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=BimanualDataset.collate_fn)\n",
    "os.makedirs('out/training-output', exist_ok=True)\n",
    "logs = Logs('out/training-output')\n",
    "new_job = logs.create_new_job(tag='example')\n",
    "\n",
    "# instantiate model\n",
    "input_size = dataset.metadata.observation_size - JOINT_OBSERVATION_SIZE  # exclude qvel observation\n",
    "output_size = dataset.metadata.action_size\n",
    "model = ExampleModel(input_size, output_size)\n",
    "if torch.cuda.is_available():\n",
    "  print('Using CUDA.')\n",
    "  model = model.cuda()\n",
    "else:\n",
    "  print('Using CPU.')\n",
    "\n",
    "# train with behavior cloning objective\n",
    "trainer = BCTrainer(dataloader, checkpoint_frequency=CHECKPOINT_FREQUENCY, job=new_job)\n",
    "trainer.train(model, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
