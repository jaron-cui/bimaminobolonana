{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31396db",
   "metadata": {},
   "source": [
    "# Examples for Behavior Cloning Dataset Generation and Usage in Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9c5e8",
   "metadata": {},
   "source": [
    "This notebook contains convenient examples demonstrating use of dataset generation and training pipeline functions.\n",
    "\n",
    "1. Start off by running the first cell below.\n",
    "2. Then, proceed in order. Some cells may be able to be skipped depending on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e52206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.getcwd()).parent.absolute()))\n",
    "\n",
    "dataset_dir_path = Path('D:/bc-train-data-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84016c",
   "metadata": {},
   "source": [
    "### Generating a Bimanual Behavior Cloning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6878f19",
   "metadata": {},
   "source": [
    "Here, we generate a dataset of (BimanualObs, BimanualAction) samples for the \"pass block\" task using policy.PrivilegedPolicy.\n",
    "This takes a while, so I've implemented convenient mechanisms for resuming data generation between sessions.\n",
    "The resulting output files can be accessed again with the BimanualDataset and HumanReadableBimanualDataset classes, as shown later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2ac692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bimanual dataset save directory is set to `D:\\bc-train-data-test`.\n",
      "Resuming from sample 9893/10000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting rollout 70.:  65%|██████▌   | 390/600 [00:38<00:20, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Rollout succeeded. Saved 107 samples at 2025-10-25 14:45:06.829538. (10000/10000)\n",
      "Finished generating 10000 samples in D:\\bc-train-data-test.\n"
     ]
    }
   ],
   "source": [
    "from train.dataset import generate_bimanual_dataset\n",
    "\n",
    "generate_bimanual_dataset(\n",
    "  save_dir=dataset_dir_path,\n",
    "  total_sample_count=10000,\n",
    "  max_steps_per_rollout=600,\n",
    "  skip_frames=2,\n",
    "  camera_dims=(128, 128),\n",
    "  resume=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af73614",
   "metadata": {},
   "source": [
    "### Viewing a Rollout from the Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: The bimanual dataset loaded from `D:\\bc-train-data-test` is incomplete, with only 4005/10000 sample slots filled.This dataset is perfectly usable, but its filespace is not completely utilized.\n"
     ]
    }
   ],
   "source": [
    "from robot.visualize import save_frames_to_video\n",
    "from train.dataset import HumanReadableBimanualDataset\n",
    "\n",
    "dataset = HumanReadableBimanualDataset(dataset_dir_path)\n",
    "rollout_number = 7\n",
    "rollout_length = dataset.metadata.rollout_lengths[rollout_number]\n",
    "rollout_start = sum(dataset.metadata.rollout_lengths[:rollout_number])\n",
    "observations = [dataset[i][0] for i in range(rollout_start, rollout_start + rollout_length)]\n",
    "\n",
    "os.makedirs('out', exist_ok=True)\n",
    "left_wrist_video_path = f'out/left_wrist_rollout_{rollout_number}.mp4'\n",
    "right_wrist_video_path = f'out/right_wrist_rollout_{rollout_number}.mp4'\n",
    "save_frames_to_video([observation.visual[0].detach().numpy() for observation in observations], left_wrist_video_path)\n",
    "save_frames_to_video([observation.visual[1].detach().numpy() for observation in observations], right_wrist_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ced06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"out/left_wrist_rollout_7.mp4\" controls  width=\"400\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(left_wrist_video_path, width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65d0e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"out/right_wrist_rollout_7.mp4\" controls  width=\"400\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(right_wrist_video_path, width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a32396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2f50e5",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba432e9",
   "metadata": {},
   "source": [
    "Here we demonstrate the suite of utilities in this `train` subpackage.\n",
    "\n",
    "1. `train.train_utils.Logs`\n",
    "  - Utility class for organizing training output files. We should use this going forward to remain organized and expand its capabilities as needed.\n",
    "2. `train.dataset.HumanReadableBimanualDataset`\n",
    "  - Dataset implementation for reading behavior cloning data.\n",
    "3. `train.trainer.BCTrainer`\n",
    "  - A class for training an arbitrary model for behavior cloning. We should more-or-less keep this class's core logic as-is.\n",
    "\n",
    "Each of these utilities currently have some TODO comments in their code - take a look. In summary, (1. Logs) doesn't currently do much logging - the Jobs class in the same file needs to have log piping implemented. (2. Dataset) currently just uses robot.sim numpy-based dataclasses for observations and actions, it should start using the torch-based versions defined in dataset.py and moving the .npy files to GPU. (3. BCTrainer) isn't very configurable right now - the optimizer and other details should be constructor args that we can dynamically configure with hydra or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532cc965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n",
      "Training model for 10 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 40/40 [02:07<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 0 loss: 504.3760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 40/40 [00:01<00:00, 31.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 1 loss: 15.9850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 40/40 [00:01<00:00, 35.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 2 loss: 11.6654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 40/40 [00:01<00:00, 34.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 3 loss: 10.7553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 40/40 [00:01<00:00, 31.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 4 loss: 10.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 40/40 [00:01<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 5 loss: 9.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 40/40 [00:01<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 6 loss: 9.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 40/40 [00:01<00:00, 30.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 7 loss: 9.4972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 40/40 [00:01<00:00, 35.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 8 loss: 8.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 40/40 [00:01<00:00, 34.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Epoch 9 loss: 9.2741\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from robot.sim import JOINT_OBSERVATION_SIZE\n",
    "from train.dataset import HumanReadableBimanualDataset, TensorBimanualAction, TensorBimanualObs\n",
    "from train.train_utils import Logs\n",
    "from train.trainer import BCTrainer, BimanualActor\n",
    "\n",
    "# example bimanual actor class\n",
    "class ExampleModel(BimanualActor):\n",
    "  def __init__(self, observation_size: int, action_size: int, hidden_size: int = 256):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(observation_size, hidden_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_size, hidden_size),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_size, action_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, obs: TensorBimanualObs) -> TensorBimanualAction:\n",
    "    x = torch.cat((obs.visual.reshape(obs.visual.shape[0], -1), obs.qpos.array), dim=-1)\n",
    "    return TensorBimanualAction(self.layers(x))\n",
    "\n",
    "# load dataset and set up trainer\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "CHECKPOINT_FREQUENCY = 1\n",
    "dataset = HumanReadableBimanualDataset(dataset_dir_path)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=HumanReadableBimanualDataset.collate_fn)\n",
    "os.makedirs('out/training-output', exist_ok=True)\n",
    "logs = Logs('out/training-output')\n",
    "new_job = logs.create_new_job(tag='example')\n",
    "\n",
    "# instantiate model\n",
    "input_size = dataset.metadata.observation_size - JOINT_OBSERVATION_SIZE  # exclude qvel observation\n",
    "output_size = dataset.metadata.action_size\n",
    "model = ExampleModel(input_size, output_size)\n",
    "if torch.cuda.is_available():\n",
    "  print('Using CUDA.')\n",
    "  model = model.cuda()\n",
    "else:\n",
    "  print('Using CPU.')\n",
    "\n",
    "# train with behavior cloning objective\n",
    "trainer = BCTrainer(dataloader, checkpoint_frequency=CHECKPOINT_FREQUENCY, job=new_job)\n",
    "trainer.train(model, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
