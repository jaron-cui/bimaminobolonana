{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate ACT Checkpoint\n",
        "\n",
        "This notebook evaluates a trained ACT checkpoint by calculating its success rate on the bimanual manipulation task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(os.getcwd()).parent.absolute()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your checkpoint path and evaluation parameters here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# ============ CONFIGURATION ============\n",
        "# Change these paths to evaluate different checkpoints\n",
        "CHECKPOINT_PATH = \"/home_shared/grail_andre/code/bimaminobolonana/runs/act_results/checkpoint/act-train/150.pt\"\n",
        "CONFIG_PATH = \"/home_shared/grail_andre/code/bimaminobolonana/runs/act_results/config.yaml\"\n",
        "\n",
        "# Evaluation parameters\n",
        "NUM_ROLLOUTS = 100  # Number of simulation rollouts to run\n",
        "MAX_STEPS_PER_ROLLOUT = 600  # Maximum steps per rollout\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from policy.act import build_act_policy\n",
        "\n",
        "# Load config\n",
        "print(f\"Loading config from: {CONFIG_PATH}\")\n",
        "config = OmegaConf.load(CONFIG_PATH)\n",
        "print(f\"\\nConfig: {config.name}\")\n",
        "print(f\"  Chunk size: {config.chunk_size}\")\n",
        "print(f\"  Temporal context: {config.temporal_context}\")\n",
        "print(f\"  Encoder: {config.encoder.name}\")\n",
        "print(f\"  Image size: {config.image_size}\")\n",
        "print(f\"  Temporal ensemble: {config.get('temporal_ensemble', False)}\")\n",
        "\n",
        "# Build model\n",
        "print(f\"\\nBuilding ACT policy...\")\n",
        "model = build_act_policy(config).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Load checkpoint\n",
        "print(f\"Loading checkpoint: {Path(CHECKPOINT_PATH).name}\")\n",
        "state_dict = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(state_dict)\n",
        "print(\"✓ Checkpoint loaded successfully!\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel parameters:\")\n",
        "print(f\"  Total: {total_params:,}\")\n",
        "print(f\"  Trainable: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Policy Wrapper\n",
        "\n",
        "Create a wrapper that handles temporal context and action chunking:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "from robot.sim import BimanualAction, BimanualObs, BimanualSim, randomize_block_position\n",
        "from train.dataset import TensorBimanualObs\n",
        "\n",
        "class ACTPolicyWrapper:\n",
        "    \"\"\"Wrapper for ACT policy that handles temporal context and action chunking.\"\"\"\n",
        "    def __init__(self, model, config, device):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.temporal_context = config.temporal_context\n",
        "        self.chunk_size = config.chunk_size\n",
        "        self.ensemble_window = config.get('ensemble_window', 10) if config.get('temporal_ensemble', False) else 1\n",
        "        self.obs_history = deque(maxlen=self.temporal_context)\n",
        "        self.action_buffer = deque(maxlen=self.ensemble_window)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset temporal context and action buffers.\"\"\"\n",
        "        self.obs_history.clear()\n",
        "        self.action_buffer.clear()\n",
        "\n",
        "    def __call__(self, obs: BimanualObs) -> BimanualAction:\n",
        "        \"\"\"Predict action from observation.\"\"\"\n",
        "        self.obs_history.append(obs)\n",
        "        while len(self.obs_history) < self.temporal_context:\n",
        "            self.obs_history.appendleft(obs)\n",
        "\n",
        "        tensor_obs = self._to_tensor_obs(list(self.obs_history))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_chunk = self.model.predict_action_chunk(tensor_obs)\n",
        "\n",
        "        self.action_buffer.append(action_chunk.cpu())\n",
        "\n",
        "        if len(self.action_buffer) > 0:\n",
        "            current_actions = []\n",
        "            for i, chunk in enumerate(self.action_buffer):\n",
        "                step_in_chunk = len(self.action_buffer) - 1 - i\n",
        "                if step_in_chunk < chunk.shape[1]:\n",
        "                    current_actions.append(chunk[0, step_in_chunk])\n",
        "\n",
        "            if current_actions:\n",
        "                action_array = torch.stack(current_actions).mean(dim=0).numpy()\n",
        "            else:\n",
        "                action_array = action_chunk[0, 0].cpu().numpy()\n",
        "        else:\n",
        "            action_array = action_chunk[0, 0].cpu().numpy()\n",
        "\n",
        "        return BimanualAction(action_array)\n",
        "\n",
        "    def _to_tensor_obs(self, obs_list):\n",
        "        \"\"\"Convert list of BimanualObs to TensorBimanualObs.\"\"\"\n",
        "        visual = np.stack([o.visual for o in obs_list], axis=0)\n",
        "        visual = torch.from_numpy(visual).float().unsqueeze(0).to(self.device)\n",
        "        qpos = torch.from_numpy(obs_list[-1].qpos.array).float().unsqueeze(0).to(self.device)\n",
        "        qvel = torch.from_numpy(obs_list[-1].qvel.array).float().unsqueeze(0).to(self.device)\n",
        "        return TensorBimanualObs(visual, qpos, qvel)\n",
        "\n",
        "# Create policy wrapper\n",
        "act_policy_wrapper = ACTPolicyWrapper(model, config, DEVICE)\n",
        "print(\"✓ Policy wrapper created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Policy and Simulation Creator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def act_policy(obs: BimanualObs) -> BimanualAction:\n",
        "    \"\"\"Policy function that wraps the ACT model.\"\"\"\n",
        "    return act_policy_wrapper(obs)\n",
        "\n",
        "def create_sim() -> BimanualSim:\n",
        "    \"\"\"Create a fresh simulation environment for each rollout.\"\"\"\n",
        "    act_policy_wrapper.reset()\n",
        "    sim = BimanualSim(\n",
        "        merge_xml_files=[Path('robot/block.xml')],\n",
        "        on_mujoco_init=randomize_block_position,\n",
        "        camera_dims=(config.image_size, config.image_size),\n",
        "        obs_camera_names=config.camera_names\n",
        "    )\n",
        "    return sim\n",
        "\n",
        "print(\"✓ Policy and simulation creator defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Full Evaluation\n",
        "\n",
        "This will run multiple rollouts and calculate the success rate. This may take a while!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validate.evaluation import evaluate_policy\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting ACT Policy Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Checkpoint: {Path(CHECKPOINT_PATH).name}\")\n",
        "print(f\"Number of rollouts: {NUM_ROLLOUTS}\")\n",
        "print(f\"Max steps per rollout: {MAX_STEPS_PER_ROLLOUT}\")\n",
        "print(f\"Temporal context: {config.temporal_context} frames\")\n",
        "print(f\"Action chunk size: {config.chunk_size}\")\n",
        "print(f\"Temporal ensemble: {config.get('temporal_ensemble', False)}\")\n",
        "if config.get('temporal_ensemble', False):\n",
        "    print(f\"Ensemble window: {config.get('ensemble_window', 10)}\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "success_rate = evaluate_policy(\n",
        "    policy=act_policy,\n",
        "    create_sim=create_sim,\n",
        "    num_rollouts=NUM_ROLLOUTS,\n",
        "    max_steps_per_rollout=MAX_STEPS_PER_ROLLOUT,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(f\"ACT Policy Success Rate: {success_rate * 100:.2f}%\")\n",
        "print(f\"Successful rollouts: {int(success_rate * NUM_ROLLOUTS)}/{NUM_ROLLOUTS}\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
