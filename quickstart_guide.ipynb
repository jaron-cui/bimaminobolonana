{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ff4fd3",
   "metadata": {},
   "source": [
    "\n",
    "# Bimaminobolonana — Quickstart Notebook\n",
    "\n",
    "This notebook mirrors the repo's README usage, but in an executable format.  \n",
    "It uses the **`encoder`** package (singular) now present in the repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b568f2a",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment setup\n",
    "\n",
    "If you're running on NYU HPC (or locally) and need a clean environment, here are two options.  \n",
    "**You don't need to execute these cells inside the notebook** if your environment is already prepared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79918111",
   "metadata": {},
   "source": [
    "\n",
    "### Option A — venv + CPU PyTorch (works everywhere)\n",
    "```bash\n",
    "python3.10 -m venv .venv && source .venv/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install --index-url https://download.pytorch.org/whl/cpu torch==2.4.1 torchvision==0.19.1\n",
    "pip install -r requirements.txt\n",
    "pytest -q\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c3838",
   "metadata": {},
   "source": [
    "\n",
    "### Option B — Conda (if you prefer)\n",
    "```bash\n",
    "conda env create -f environment.yaml\n",
    "conda activate dev\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c76cf9",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cff5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, yaml\n",
    "from PIL import Image\n",
    "from encoder import build_encoder\n",
    "from encoder.transforms import build_image_transform, prepare_batch\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de10cb",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Transforms (input preprocessing)\n",
    "\n",
    "Helpers to get correctly sized/normalized tensors for each encoder.\n",
    "- `kind=\"clip\"` for CLIP\n",
    "- `kind=\"imagenet\"` for Pri3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e45cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CLIP-style preprocessing (use for clip_vit)\n",
    "tfm_clip = build_image_transform(kind=\"clip\", size=224)\n",
    "# ImageNet-style preprocessing (use for pri3d)\n",
    "tfm_im = build_image_transform(kind=\"imagenet\", size=224)\n",
    "print(\"Transforms ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3833ac52",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Encoder skeleton quickstart\n",
    "\n",
    "`build_encoder(cfg)` returns an object with `encode((left, right)) -> dict`  \n",
    "containing `left`, `right`, and `fused` features (all with shape `B×512` by default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6792e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal config (CLIP stub; no weights) — uses the repo's YAML if present.\n",
    "try:\n",
    "    with open(\"configs/encoder_clip_b32.yaml\", \"r\") as f:\n",
    "        cfg_clip = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    cfg_clip = {\"name\":\"clip_vit\",\"model_name\":\"ViT-B-32\",\"out_dim\":512,\"freeze\":True,\"fuse\":\"mean\"}\n",
    "\n",
    "enc_clip = build_encoder(cfg_clip)\n",
    "# Create two dummy RGB images and batch them\n",
    "left_img  = Image.new(\"RGB\", (320, 240), color=(255, 0, 0))\n",
    "right_img = Image.new(\"RGB\", (240, 320), color=(0, 255, 0))\n",
    "x_left  = prepare_batch(left_img,  transform=tfm_clip)\n",
    "x_right = prepare_batch(right_img, transform=tfm_clip)\n",
    "out = enc_clip.encode((x_left, x_right))\n",
    "{k: v.shape for k, v in out.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccf87a",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1) CLIP (pretrained via open-clip)\n",
    "\n",
    "If you have `configs/encoder_clip_b32_openai.yaml`, this cell will use it; otherwise it falls back to the stub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc84d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "cfg_path = Path(\"configs/encoder_clip_b32_openai.yaml\")\n",
    "if cfg_path.exists():\n",
    "    with open(cfg_path, \"r\") as f:\n",
    "        cfg_clip_openai = yaml.safe_load(f)\n",
    "    enc_clip_openai = build_encoder(cfg_clip_openai)\n",
    "    out_openai = enc_clip_openai.encode((x_left, x_right))\n",
    "    print({k: v.shape for k, v in out_openai.items()})\n",
    "else:\n",
    "    print(\"configs/encoder_clip_b32_openai.yaml not found — skipping pretrained CLIP demo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813b0e4",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Pri3D encoder (random-init ResNet)\n",
    "\n",
    "Capacity-matched control using torchvision ResNet (18/34/50), random initialization, ImageNet preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    with open(\"configs/encoder_pri3d_random.yaml\", \"r\") as f:\n",
    "        cfg_pri = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    cfg_pri = {\"name\":\"pri3d\",\"variant\":\"resnet50\",\"pretrained\":False,\"freeze\":False,\"out_dim\":512,\"fuse\":\"mean\"}\n",
    "\n",
    "enc_pri = build_encoder(cfg_pri)\n",
    "# Reuse the ImageNet transform\n",
    "left_img2  = Image.new(\"RGB\", (320, 240), color=(30, 30, 200))\n",
    "right_img2 = Image.new(\"RGB\", (240, 320), color=(30, 200, 30))\n",
    "x_left2  = prepare_batch(left_img2,  transform=tfm_im)\n",
    "x_right2 = prepare_batch(right_img2, transform=tfm_im)\n",
    "out2 = enc_pri.encode((x_left2, x_right2))\n",
    "{k: v.shape for k, v in out2.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f631b",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1) Pri3D (pretrained)\n",
    "\n",
    "If you have a Pri3D checkpoint (e.g., the authors' **ResNet-50 Pri3D (View+Geo)**), point the config to it:\n",
    "```yaml\n",
    "name: pri3d\n",
    "variant: resnet50\n",
    "pretrained: true\n",
    "ckpt_path: /path/to/ScanNet_Combine_BatchSize64_LearningRate01_Epoch5_ImageSize240x320_ResNet50.pth\n",
    "freeze: true\n",
    "out_dim: 512\n",
    "fuse: mean\n",
    "```\n",
    "\n",
    "This cell will try to use `configs/encoder_pri3d_pretrained.yaml` if it exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg_pre = Path(\"configs/encoder_pri3d_pretrained.yaml\")\n",
    "if cfg_pre.exists():\n",
    "    cfg = yaml.safe_load(cfg_pre.read_text())\n",
    "    if cfg.get(\"ckpt_path\") and Path(cfg[\"ckpt_path\"]).expanduser().exists():\n",
    "        enc_pri_pre = build_encoder(cfg)\n",
    "        outp = enc_pri_pre.encode((x_left2, x_right2))\n",
    "        print({k: v.shape for k, v in outp.items()})\n",
    "    else:\n",
    "        print(\"configs/encoder_pri3d_pretrained.yaml found but ckpt_path missing/not found — skipping.\")\n",
    "else:\n",
    "    print(\"configs/encoder_pri3d_pretrained.yaml not found — skipping Pri3D pretrained demo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b920fc",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Notes\n",
    "- All encoders return `B×512` features and a `fused` vector when `fuse` is set.\n",
    "- Use `build_image_transform(kind=...)` to match the encoder's expected normalization.\n",
    "- For reproducible CI, prefer CPU Torch + the stub configs; for experiments, use pretrained CLIP and Pri3D.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}